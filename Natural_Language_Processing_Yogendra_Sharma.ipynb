{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "YK9lUEhCSIZX",
        "h2Lzkwoh1xQO",
        "BXIXLj5F2wg5",
        "Xkd_GnHId4yI",
        "-_G7Kl0Mebaw",
        "vw063yH0lP88",
        "Dhwb6ycEN5x4",
        "nq1fRz0fS7Ds",
        "ZEtzN2B-VaJh",
        "Hqy84VuqjSCt",
        "lSSqPXwMj9jF",
        "P1hCWdqYkS7k",
        "FpCrGdeSsMTm",
        "GKz9A0FR4Hy2",
        "bBwXaZCvsswZ"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "##**Natural Language Processing Project**\n",
        "\n",
        "> Indented block\n",
        "\n"
      ],
      "metadata": {
        "id": "jPzn42MS0t1s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd"
      ],
      "metadata": {
        "id": "QFQUh81s1WzZ"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**Part A**"
      ],
      "metadata": {
        "id": "1hc0_1MJlRcS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qYxkgd3IveUv",
        "outputId": "02a171d0-e8e2-460f-efac-5e45c6c5c98f"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**1. Steps and tasks:.**"
      ],
      "metadata": {
        "id": "YK9lUEhCSIZX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "####1. Import and analyse the data set.\n",
        "Hint: - Use `imdb.load_data()` method\n",
        "- Get train and test set\n",
        "- Take 10000 most frequent words"
      ],
      "metadata": {
        "id": "h2Lzkwoh1xQO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Import necessary libraries:**"
      ],
      "metadata": {
        "id": "1yQHO7EAwAyQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np"
      ],
      "metadata": {
        "id": "eKaYOZOwwZPU"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.datasets import imdb"
      ],
      "metadata": {
        "id": "o8AgiNt8we1b"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip uninstall tensorflow tensorflow-estimator"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0wE3xC0OyAmf",
        "outputId": "4c03c3e8-4a6b-45ff-f318-610ac21e09fb"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found existing installation: tensorflow 2.13.0\n",
            "Uninstalling tensorflow-2.13.0:\n",
            "  Would remove:\n",
            "    /usr/local/bin/estimator_ckpt_converter\n",
            "    /usr/local/bin/import_pb_to_tensorboard\n",
            "    /usr/local/bin/saved_model_cli\n",
            "    /usr/local/bin/tensorboard\n",
            "    /usr/local/bin/tf_upgrade_v2\n",
            "    /usr/local/bin/tflite_convert\n",
            "    /usr/local/bin/toco\n",
            "    /usr/local/bin/toco_from_protos\n",
            "    /usr/local/lib/python3.10/dist-packages/tensorflow-2.13.0.dist-info/*\n",
            "    /usr/local/lib/python3.10/dist-packages/tensorflow/*\n",
            "Proceed (Y/n)? Y\n",
            "  Successfully uninstalled tensorflow-2.13.0\n",
            "Found existing installation: tensorflow-estimator 2.13.0\n",
            "Uninstalling tensorflow-estimator-2.13.0:\n",
            "  Would remove:\n",
            "    /usr/local/lib/python3.10/dist-packages/tensorflow_estimator-2.13.0.dist-info/*\n",
            "    /usr/local/lib/python3.10/dist-packages/tensorflow_estimator/*\n",
            "Proceed (Y/n)? Y\n",
            "  Successfully uninstalled tensorflow-estimator-2.13.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install --upgrade typing-extensions"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wuEz0MRgxIOj",
        "outputId": "d46c9335-218c-4def-fe40-f91b1513c3f4"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (4.7.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install tensorflow"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "9XIee-elyUcx",
        "outputId": "a2dab1fb-6991-49fb-ab91-173bf85a0a6b"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting tensorflow\n",
            "  Using cached tensorflow-2.13.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (524.1 MB)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=23.1.21 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (23.5.26)\n",
            "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.4.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.57.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.9.0)\n",
            "Requirement already satisfied: keras<2.14,>=2.13.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.13.1)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (16.0.6)\n",
            "Requirement already satisfied: numpy<=1.24.3,>=1.22 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.23.5)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.3.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow) (23.1)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.20.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow) (67.7.2)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.16.0)\n",
            "Requirement already satisfied: tensorboard<2.14,>=2.13 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.13.0)\n",
            "Collecting tensorflow-estimator<2.14,>=2.13.0 (from tensorflow)\n",
            "  Using cached tensorflow_estimator-2.13.0-py2.py3-none-any.whl (440 kB)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.3.0)\n",
            "Collecting typing-extensions<4.6.0,>=3.6.6 (from tensorflow)\n",
            "  Using cached typing_extensions-4.5.0-py3-none-any.whl (27 kB)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.14.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.33.0)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow) (0.41.2)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.14,>=2.13->tensorflow) (2.17.3)\n",
            "Requirement already satisfied: google-auth-oauthlib<1.1,>=0.5 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.14,>=2.13->tensorflow) (1.0.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.14,>=2.13->tensorflow) (3.4.4)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.14,>=2.13->tensorflow) (2.31.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.14,>=2.13->tensorflow) (0.7.1)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.14,>=2.13->tensorflow) (2.3.7)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow) (5.3.1)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow) (0.3.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<1.1,>=0.5->tensorboard<2.14,>=2.13->tensorflow) (1.3.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.14,>=2.13->tensorflow) (3.2.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.14,>=2.13->tensorflow) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.14,>=2.13->tensorflow) (2.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.14,>=2.13->tensorflow) (2023.7.22)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.14,>=2.13->tensorflow) (2.1.3)\n",
            "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow) (0.5.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard<2.14,>=2.13->tensorflow) (3.2.2)\n",
            "Installing collected packages: typing-extensions, tensorflow-estimator, tensorflow\n",
            "  Attempting uninstall: typing-extensions\n",
            "    Found existing installation: typing_extensions 4.7.1\n",
            "    Uninstalling typing_extensions-4.7.1:\n",
            "      Successfully uninstalled typing_extensions-4.7.1\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "pydantic 2.2.1 requires typing-extensions>=4.6.1, but you have typing-extensions 4.5.0 which is incompatible.\n",
            "pydantic-core 2.6.1 requires typing-extensions!=4.7.0,>=4.6.0, but you have typing-extensions 4.5.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed tensorflow-2.13.0 tensorflow-estimator-2.13.0 typing-extensions-4.5.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "tensorflow"
                ]
              }
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.preprocessing.sequence import pad_sequences"
      ],
      "metadata": {
        "id": "r-Vx_Twcwj4L"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.layers import Embedding, LSTM, Dense, Dropout"
      ],
      "metadata": {
        "id": "_47Y6wtTwoXt"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.models import Sequential"
      ],
      "metadata": {
        "id": "WX7sNwBpwrmu"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.utils import plot_model"
      ],
      "metadata": {
        "id": "Wic7B6R3wvKT"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Load the IMDb dataset:**"
      ],
      "metadata": {
        "id": "V-bmZ47ozIt9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the IMDb dataset and limit it to the 10,000 most frequent words\n",
        "vocab_size = 10000\n",
        "(train_data, train_labels), (test_data, test_labels) = imdb.load_data(num_words=vocab_size)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W2tEdUJvU6Dg",
        "outputId": "b271838e-dbab-4948-ac54-a1fa1d2c3898"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/imdb.npz\n",
            "17464789/17464789 [==============================] - 0s 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Preprocess the data:**"
      ],
      "metadata": {
        "id": "jbTxa-j1zKVd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.datasets import imdb"
      ],
      "metadata": {
        "id": "9kzMBB3T0H8k"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the IMDb dataset and limit it to the 10,000 most frequent words\n",
        "vocab_size = 10000\n",
        "(train_data, train_labels), (test_data, test_labels) = imdb.load_data(num_words=vocab_size)"
      ],
      "metadata": {
        "id": "8VfbiBe1U-6Y"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Load GloVe word embeddings:**"
      ],
      "metadata": {
        "id": "69O3CQmn0RJL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np"
      ],
      "metadata": {
        "id": "pajeobPY11Am"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Load GloVe embeddings with 'latin-1' encoding\n",
        "embedding_index = {}\n",
        "with open(glove_path, 'r', encoding='latin-1') as file:\n",
        "    for line in file:\n",
        "        values = line.split()\n",
        "        if len(values) < embedding_dim + 1:\n",
        "            continue  # Skip lines with improper formatting\n",
        "        word = values[0]\n",
        "        try:\n",
        "            coefs = np.asarray(values[1:], dtype='float32')\n",
        "            embedding_index[word] = coefs\n",
        "        except ValueError:\n",
        "            continue  # Skip lines that cannot be converted to floats\n",
        "\n",
        "# Now, you should have the embedding_index populated with word embeddings"
      ],
      "metadata": {
        "id": "udj2wpMf13nm"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Load GloVe embeddings with 'latin-1' encoding\n",
        "embedding_index = {}\n",
        "with open(glove_path, 'r', encoding='latin-1') as file:\n",
        "    for line in file:\n",
        "        values = line.split()\n",
        "        if len(values) < embedding_dim + 1:\n",
        "            continue  # Skip lines with improper formatting\n",
        "        word = values[0]\n",
        "        try:\n",
        "            coefs = np.asarray(values[1:], dtype='float32')\n",
        "            embedding_index[word] = coefs\n",
        "        except ValueError:\n",
        "            continue  # Skip lines that cannot be converted to floats\n",
        "\n",
        "# Now, you should have the embedding_index populated with word embeddings"
      ],
      "metadata": {
        "id": "85qep0D222Gc"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Load GloVe embeddings with 'ISO-8859-15' encoding\n",
        "embedding_index = {}\n",
        "with open(glove_path, 'r', encoding='ISO-8859-15') as file:\n",
        "    for line in file:\n",
        "        values = line.split()\n",
        "        if len(values) < embedding_dim + 1:\n",
        "            continue  # Skip lines with improper formatting\n",
        "        word = values[0]\n",
        "        try:\n",
        "            coefs = np.asarray(values[1:], dtype='float32')\n",
        "            embedding_index[word] = coefs\n",
        "        except ValueError:\n",
        "            continue  # Skip lines that cannot be converted to floats\n",
        "\n",
        "# Now, you should have the embedding_index populated with word embeddings"
      ],
      "metadata": {
        "id": "7Fwko4Nh3JO2"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Path to the GloVe embedding file\n",
        "glove_path = '/content/drive/MyDrive/Projects/Project/Project-08/glove.6B.zip'  # Adjust the path as needed\n",
        "\n",
        "# Load GloVe embeddings with 'ISO-8859-15' encoding\n",
        "embedding_dim = 100  # Adjust the dimension according to the downloaded GloVe file\n",
        "embedding_index = {}\n",
        "with open(glove_path, 'r', encoding='ISO-8859-15') as file:\n",
        "    for line in file:\n",
        "        values = line.split()\n",
        "        if len(values) < embedding_dim + 1:\n",
        "            continue  # Skip lines with improper formatting\n",
        "        word = values[0]\n",
        "        try:\n",
        "            coefs = np.asarray(values[1:], dtype='float32')\n",
        "            embedding_index[word] = coefs\n",
        "        except ValueError:\n",
        "            continue  # Skip lines that cannot be converted to floats\n",
        "\n",
        "# Create an embedding matrix for the IMDb dataset\n",
        "embedding_matrix = np.zeros((vocab_size, embedding_dim))\n",
        "for word, i in imdb.get_word_index().items():\n",
        "    if i < vocab_size:\n",
        "        embedding_vector = embedding_index.get(word)\n",
        "        if embedding_vector is not None:\n",
        "            embedding_matrix[i] = embedding_vector"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B-PLrFud0ykP",
        "outputId": "a617dbc7-28fa-4774-e71a-2f9ceeb98237"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/imdb_word_index.json\n",
            "1641221/1641221 [==============================] - 0s 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Define and compile a neural network model:**"
      ],
      "metadata": {
        "id": "qU6tRRM14Fx7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = Sequential()\n",
        "model.add(Embedding(vocab_size, embedding_dim, input_length=max_sequence_length, weights=[embedding_matrix], trainable=False))\n",
        "model.add(LSTM(64))\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "iUfwBzy74NIl"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Train the model:**"
      ],
      "metadata": {
        "id": "PpsFGHIP4SdN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "train_data = np.array(train_data)"
      ],
      "metadata": {
        "id": "IIqPddui5YeN"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.utils import to_categorical\n",
        "\n",
        "train_labels = to_categorical(train_labels)"
      ],
      "metadata": {
        "id": "2J-HAHe25bds"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(type(train_data))\n",
        "print(train_data.shape)\n",
        "\n",
        "print(type(train_labels))\n",
        "print(train_labels.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8g6N7fa6_UgR",
        "outputId": "e9df1eda-85b4-45e3-b781-80800252ea7f"
      },
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'numpy.ndarray'>\n",
            "(25000,)\n",
            "<class 'numpy.ndarray'>\n",
            "(25000, 2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Evaluate the model:**"
      ],
      "metadata": {
        "id": "FIQVMPJp6dJ7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "test_data = np.array(test_data)"
      ],
      "metadata": {
        "id": "o7wCs4cF7Irg"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.utils import to_categorical\n",
        "\n",
        "test_labels = to_categorical(test_labels)"
      ],
      "metadata": {
        "id": "lyUIBObj7LRx"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.add(Dense(1, activation='sigmoid'))"
      ],
      "metadata": {
        "id": "U6U_hx6S8D3S"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Assuming test_labels is a list of one-hot encoded labels\n",
        "# Convert one-hot encoded labels to binary labels\n",
        "test_labels = np.argmax(test_labels, axis=1)"
      ],
      "metadata": {
        "id": "axaXsQ0F8LMl"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "\n",
        "# Define and compile your model outside of the decorated function\n",
        "model = tf.keras.Sequential()\n",
        "model.add(tf.keras.layers.Embedding(input_dim=vocab_size, output_dim=embedding_dim, input_length=max_sequence_length))\n",
        "model.add(tf.keras.layers.Flatten())  # For example, you can use Flatten to prepare for dense layers\n",
        "model.add(tf.keras.layers.Dense(1, activation='sigmoid'))  # Output layer for binary classification\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Define a decorated function for evaluation\n",
        "@tf.function\n",
        "def evaluate_model(test_data):\n",
        "    # Use the pre-defined model for evaluation\n",
        "    predictions = model(test_data)\n",
        "    return predictions\n",
        "\n",
        "# Assuming test_data is properly formatted (you've already converted it)\n",
        "predictions = evaluate_model(test_data)\n",
        "\n",
        "# You can post-process the predictions as needed for your evaluation\n",
        "# For example, calculate accuracy based on predictions and test_labels\n",
        "test_labels = np.array(test_labels)  # Assuming test_labels is properly formatted as a NumPy array\n",
        "accuracy = np.mean((predictions > 0.5).numpy().astype(int) == test_labels)\n",
        "\n",
        "print(f'Test accuracy: {accuracy * 100:.2f}%')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gjWWcMCV6hQL",
        "outputId": "2d459656-8281-4145-aa3e-934cbc6e3d3a"
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test accuracy: 50.00%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "####2. Perform relevant sequence adding on the data."
      ],
      "metadata": {
        "id": "BXIXLj5F2wg5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "# Define the maximum sequence length (adjust as needed)\n",
        "max_sequence_length = 500\n",
        "\n",
        "# Pad sequences with zeros or trim sequences to the specified length\n",
        "train_data = pad_sequences(train_data, maxlen=max_sequence_length)\n",
        "test_data = pad_sequences(test_data, maxlen=max_sequence_length)"
      ],
      "metadata": {
        "id": "AvHNeb3H22oc"
      },
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "####3. Perform following data analysis:\n",
        "• Print shape of features and labels\n",
        "\n",
        "• Print value of any one feature and it's label"
      ],
      "metadata": {
        "id": "Xkd_GnHId4yI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Print the shape of features and labels\n",
        "print(\"Shape of train_data:\", train_data.shape)\n",
        "print(\"Shape of train_labels:\", train_labels.shape)\n",
        "\n",
        "# Print the value of one feature and its corresponding label\n",
        "sample_feature = train_data[0]  # Change the index as needed\n",
        "sample_label = train_labels[0]  # Change the index as needed\n",
        "\n",
        "print(\"Sample Feature:\")\n",
        "print(sample_feature)\n",
        "\n",
        "print(\"Corresponding Label:\")\n",
        "print(sample_label)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TREUCyzydj6Q",
        "outputId": "eac9faae-ad48-4a9a-e52f-f950aa8fdbe0"
      },
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape of train_data: (25000, 500)\n",
            "Shape of train_labels: (25000, 2)\n",
            "Sample Feature:\n",
            "[   0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    1   14   22   16   43  530  973 1622 1385   65  458 4468\n",
            "   66 3941    4  173   36  256    5   25  100   43  838  112   50  670\n",
            "    2    9   35  480  284    5  150    4  172  112  167    2  336  385\n",
            "   39    4  172 4536 1111   17  546   38   13  447    4  192   50   16\n",
            "    6  147 2025   19   14   22    4 1920 4613  469    4   22   71   87\n",
            "   12   16   43  530   38   76   15   13 1247    4   22   17  515   17\n",
            "   12   16  626   18    2    5   62  386   12    8  316    8  106    5\n",
            "    4 2223 5244   16  480   66 3785   33    4  130   12   16   38  619\n",
            "    5   25  124   51   36  135   48   25 1415   33    6   22   12  215\n",
            "   28   77   52    5   14  407   16   82    2    8    4  107  117 5952\n",
            "   15  256    4    2    7 3766    5  723   36   71   43  530  476   26\n",
            "  400  317   46    7    4    2 1029   13  104   88    4  381   15  297\n",
            "   98   32 2071   56   26  141    6  194 7486   18    4  226   22   21\n",
            "  134  476   26  480    5  144   30 5535   18   51   36   28  224   92\n",
            "   25  104    4  226   65   16   38 1334   88   12   16  283    5   16\n",
            " 4472  113  103   32   15   16 5345   19  178   32]\n",
            "Corresponding Label:\n",
            "[0. 1.]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove duplicate records and keep the first occurrence\n",
        "df_cleaned = df.drop_duplicates(keep='first')"
      ],
      "metadata": {
        "id": "mgEHqkqfV_YO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "####4. Decode the feature value to get original sentence"
      ],
      "metadata": {
        "id": "-_G7Kl0Mebaw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Reverse mapping from word indices to words\n",
        "word_index = imdb.get_word_index()\n",
        "reverse_word_index = dict([(value, key) for (key, value) in word_index.items()])\n",
        "\n",
        "# Function to decode a sequence of indices into a sentence\n",
        "def decode_feature(sequence):\n",
        "    return ' '.join([reverse_word_index.get(i - 3, '?') for i in sequence])\n",
        "\n",
        "# Decode the sample feature\n",
        "original_sentence = decode_feature(sample_feature)\n",
        "\n",
        "# Print the original sentence\n",
        "print(\"Original Sentence:\")\n",
        "print(original_sentence)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RdjomaAUkYoP",
        "outputId": "33bcbef2-5c03-420c-c51f-44d6913bf43b"
      },
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original Sentence:\n",
            "? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? this film was just brilliant casting location scenery story direction everyone's really suited the part they played and you could just imagine being there robert ? is an amazing actor and now the same being director ? father came from the same scottish island as myself so i loved the fact there was a real connection with this film the witty remarks throughout the film were great it was just brilliant so much that i bought the film as soon as it was released for ? and would recommend it to everyone to watch and the fly fishing was amazing really cried at the end it was so sad and you know what they say if you cry at a film it must have been good and this definitely was also ? to the two little boy's that played the ? of norman and paul they were just brilliant children are often left out of the ? list i think because the stars that play them all grown up are such a big profile for the whole film but these children are amazing and should be praised for what they have done don't you think the whole story was so lovely because it was true and was someone's life after all that was shared with us all\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "####5. Design, train, tune and test a sequential model.\n",
        "Hint: The aim here Is to import the text, process it such a way that it can be taken as an inout to the ML/NN classifiers. Be\n",
        "analytical and experimental here in trying new approaches to design the best model."
      ],
      "metadata": {
        "id": "vw063yH0lP88"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 1: Preprocessing the Text Data**\n",
        "\n",
        "Before you can feed the text data into a neural network, you need to preprocess it. Here are some common preprocessing steps:\n",
        "\n",
        "**Tokenization:** Splitting the text into words or subword tokens.\n",
        "\n",
        "**Padding:** Ensuring that all input sequences have the same length.\n",
        "\n",
        "**Word Embeddings:** Converting words or tokens into vector representations.\n",
        "\n",
        "**Label Encoding:** Converting class labels into numerical values.\n",
        "Here's a sample code for preprocessing:"
      ],
      "metadata": {
        "id": "q5qeq2MMBgx3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "# Sample data, replace with your actual training and testing text data\n",
        "train_text = [\"your training text data here\"]\n",
        "test_text = [\"your testing text data here\"]\n",
        "train_labels = [\"label1\", \"label2\"]  # Replace with your actual training labels\n",
        "test_labels = [\"label1\", \"label2\"]    # Replace with your actual testing labels\n",
        "\n",
        "# Tokenization and padding\n",
        "max_sequence_length = 500  # Adjust this based on your data\n",
        "vocab_size = 10000  # Adjust based on your requirements\n",
        "tokenizer = Tokenizer(num_words=vocab_size)\n",
        "tokenizer.fit_on_texts(train_text)\n",
        "train_sequences = tokenizer.texts_to_sequences(train_text)\n",
        "test_sequences = tokenizer.texts_to_sequences(test_text)\n",
        "train_data = pad_sequences(train_sequences, maxlen=max_sequence_length)\n",
        "test_data = pad_sequences(test_sequences, maxlen=max_sequence_length)\n",
        "\n",
        "# Label encoding\n",
        "label_encoder = LabelEncoder()\n",
        "train_labels = label_encoder.fit_transform(train_labels)\n",
        "test_labels = label_encoder.transform(test_labels)"
      ],
      "metadata": {
        "id": "SZWWapUGlVj8"
      },
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 2: Designing the Sequential Model**"
      ],
      "metadata": {
        "id": "qzmp_HAiCBZr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Embedding, LSTM, Dense\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Embedding(input_dim=vocab_size, output_dim=embedding_dim, input_length=max_sequence_length))\n",
        "model.add(LSTM(128, return_sequences=True))\n",
        "model.add(LSTM(64))\n",
        "model.add(Dense(1, activation='sigmoid'))  # Assuming binary classification\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "YlXqAP2yldnN"
      },
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 3: Training and Tuning the Model**"
      ],
      "metadata": {
        "id": "qxlAkatvCIRE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a list to store the text data\n",
        "texts = []\n",
        "\n",
        "# Specify the path to your text file\n",
        "file_path = \"/content/drive/MyDrive/Projects/Project/Project-08/glove.6B/glove.6B.100d.txt\"\n",
        "\n",
        "# Open the file and read its contents\n",
        "with open(file_path, \"r\") as file:\n",
        "    for line in file:\n",
        "        # Assuming each line in the file represents a piece of text\n",
        "        # You can modify this logic based on the structure of your text file\n",
        "        text = line.strip()  # Remove leading/trailing whitespace if needed\n",
        "        texts.append(text)\n",
        "\n",
        "# Now, the 'texts' list contains your actual text data"
      ],
      "metadata": {
        "id": "u8PCOfenD1Xk"
      },
      "execution_count": 75,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 4: Evaluating the Model**"
      ],
      "metadata": {
        "id": "yJil7v4yGx3n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Length of test_data:\", len(test_data))\n",
        "print(\"Length of test_labels:\", len(test_labels))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Eiz25F2sHw5v",
        "outputId": "1a032fbd-6956-4a0d-8810-1a04013ec8fd"
      },
      "execution_count": 81,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Length of test_data: 1\n",
            "Length of test_labels: 2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "print(\"NaN values in test_data:\", np.isnan(test_data).any())\n",
        "print(\"Infinite values in test_data:\", np.isinf(test_data).any())\n",
        "print(\"NaN values in test_labels:\", np.isnan(test_labels).any())\n",
        "print(\"Infinite values in test_labels:\", np.isinf(test_labels).any())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zn5BSxJxJQ3A",
        "outputId": "469ce40f-7b30-40ab-9e75-1f9024ef282b"
      },
      "execution_count": 88,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NaN values in test_data: False\n",
            "Infinite values in test_data: False\n",
            "NaN values in test_labels: False\n",
            "Infinite values in test_labels: False\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate the model on the test data\n",
        "evaluation_result = model.evaluate(test_data, test_labels, verbose=0)\n",
        "\n",
        "# The evaluation result is a list, with the first value being the loss and the second being the accuracy (if applicable)\n",
        "if len(evaluation_result) >= 2:\n",
        "    loss, accuracy = evaluation_result[0], evaluation_result[1]\n",
        "    print(f'Test loss: {loss}')\n",
        "    print(f'Test accuracy: {accuracy}')\n",
        "else:\n",
        "    print(\"Unable to retrieve loss and accuracy from evaluation result.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BGo6UKK9JAB3",
        "outputId": "febc3791-01e0-4478-e4f2-f718fc156c98"
      },
      "execution_count": 93,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Unable to retrieve loss and accuracy from evaluation result.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "####6.Use the designed model to print the prediction on any one sample."
      ],
      "metadata": {
        "id": "Dhwb6ycEN5x4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "import numpy as np\n",
        "\n",
        "# Specify the URL to your test data file\n",
        "test_data_url = \"/content/drive/MyDrive/Projects/Project/Project-08/glove.6B/glove.6B.100d.txt\"\n",
        "\n",
        "# Load your test data from a URL\n",
        "# Replace 'your_test_data.txt' with the actual file path or URL\n",
        "test_data = []\n",
        "with open(test_data_url, 'r', encoding='utf-8') as file:\n",
        "    for line in file:\n",
        "        test_data.append(line.strip())  # Assuming each line is a piece of text\n",
        "\n",
        "# Tokenize the test data\n",
        "# Assuming you have previously fit a tokenizer on your training data\n",
        "tokenizer = Tokenizer(num_words=vocab_size)  # Specify vocab_size\n",
        "tokenizer.fit_on_texts(test_data)  # Fit tokenizer on test data\n",
        "\n",
        "# Convert test data to sequences\n",
        "test_sequences = tokenizer.texts_to_sequences(test_data)\n",
        "\n",
        "# Pad sequences to a fixed length (adjust maxlen as needed)\n",
        "maxlen = 100  # Example: Set to the desired sequence length\n",
        "test_data_padded = pad_sequences(test_sequences, maxlen=maxlen, padding='post', truncating='post')\n",
        "\n",
        "# Now, test_data_padded contains your preprocessed test data ready for evaluation"
      ],
      "metadata": {
        "id": "2UEOO3PlR91f"
      },
      "execution_count": 98,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**Part B:**"
      ],
      "metadata": {
        "id": "QsjcbuSgXjDT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "####1. Read and explore the data"
      ],
      "metadata": {
        "id": "nq1fRz0fS7Ds"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Specify the path to your GloVe word embedding file\n",
        "glove_file_path = '/content/drive/MyDrive/Projects/Project/Project-08/glove.6B/glove.6B.300d.txt'\n",
        "\n",
        "# Open the GloVe file and explore its contents\n",
        "with open(glove_file_path, 'r', encoding='utf-8') as glove_file:\n",
        "    # Read and display the first few lines (word vectors)\n",
        "    for i, line in enumerate(glove_file):\n",
        "        if i < 5:  # Print the first 5 lines for illustration\n",
        "            print(line)\n",
        "        else:\n",
        "            break\n",
        "\n",
        "    # Count the total number of word vectors in the file\n",
        "    num_word_vectors = sum(1 for line in glove_file)\n",
        "\n",
        "# Display the total number of word vectors in the GloVe file\n",
        "print(f\"Total number of word vectors: {num_word_vectors}\")"
      ],
      "metadata": {
        "id": "geREEMoXgbR4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bfd8cfa9-f5e6-4e21-9a4c-101b978dcb78"
      },
      "execution_count": 102,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "the 0.04656 0.21318 -0.0074364 -0.45854 -0.035639 0.23643 -0.28836 0.21521 -0.13486 -1.6413 -0.26091 0.032434 0.056621 -0.043296 -0.021672 0.22476 -0.075129 -0.067018 -0.14247 0.038825 -0.18951 0.29977 0.39305 0.17887 -0.17343 -0.21178 0.23617 -0.063681 -0.42318 -0.11661 0.093754 0.17296 -0.33073 0.49112 -0.68995 -0.092462 0.24742 -0.17991 0.097908 0.083118 0.15299 -0.27276 -0.038934 0.54453 0.53737 0.29105 -0.0073514 0.04788 -0.4076 -0.026759 0.17919 0.010977 -0.10963 -0.26395 0.07399 0.26236 -0.1508 0.34623 0.25758 0.11971 -0.037135 -0.071593 0.43898 -0.040764 0.016425 -0.4464 0.17197 0.046246 0.058639 0.041499 0.53948 0.52495 0.11361 -0.048315 -0.36385 0.18704 0.092761 -0.11129 -0.42085 0.13992 -0.39338 -0.067945 0.12188 0.16707 0.075169 -0.015529 -0.19499 0.19638 0.053194 0.2517 -0.34845 -0.10638 -0.34692 -0.19024 -0.2004 0.12154 -0.29208 0.023353 -0.11618 -0.35768 0.062304 0.35884 0.02906 0.0073005 0.0049482 -0.15048 -0.12313 0.19337 0.12173 0.44503 0.25147 0.10781 -0.17716 0.038691 0.08153 0.14667 0.063666 0.061332 -0.075569 -0.37724 0.01585 -0.30342 0.28374 -0.042013 -0.040715 -0.15269 0.07498 0.15577 0.10433 0.31393 0.19309 0.19429 0.15185 -0.10192 -0.018785 0.20791 0.13366 0.19038 -0.25558 0.304 -0.01896 0.20147 -0.4211 -0.0075156 -0.27977 -0.19314 0.046204 0.19971 -0.30207 0.25735 0.68107 -0.19409 0.23984 0.22493 0.65224 -0.13561 -0.17383 -0.048209 -0.1186 0.0021588 -0.019525 0.11948 0.19346 -0.4082 -0.082966 0.16626 -0.10601 0.35861 0.16922 0.07259 -0.24803 -0.10024 -0.52491 -0.17745 -0.36647 0.2618 -0.012077 0.08319 -0.21528 0.41045 0.29136 0.30869 0.078864 0.32207 -0.041023 -0.1097 -0.092041 -0.12339 -0.16416 0.35382 -0.082774 0.33171 -0.24738 -0.048928 0.15746 0.18988 -0.026642 0.063315 -0.010673 0.34089 1.4106 0.13417 0.28191 -0.2594 0.055267 -0.052425 -0.25789 0.019127 -0.022084 0.32113 0.068818 0.51207 0.16478 -0.20194 0.29232 0.098575 0.013145 -0.10652 0.1351 -0.045332 0.20697 -0.48425 -0.44706 0.0033305 0.0029264 -0.10975 -0.23325 0.22442 -0.10503 0.12339 0.10978 0.048994 -0.25157 0.40319 0.35318 0.18651 -0.023622 -0.12734 0.11475 0.27359 -0.21866 0.015794 0.81754 -0.023792 -0.85469 -0.16203 0.18076 0.028014 -0.1434 0.0013139 -0.091735 -0.089704 0.11105 -0.16703 0.068377 -0.087388 -0.039789 0.014184 0.21187 0.28579 -0.28797 -0.058996 -0.032436 -0.0047009 -0.17052 -0.034741 -0.11489 0.075093 0.099526 0.048183 -0.073775 -0.41817 0.0041268 0.44414 -0.16062 0.14294 -2.2628 -0.027347 0.81311 0.77417 -0.25639 -0.11576 -0.11982 -0.21363 0.028429 0.27261 0.031026 0.096782 0.0067769 0.14082 -0.013064 -0.29686 -0.079913 0.195 0.031549 0.28506 -0.087461 0.0090611 -0.20989 0.053913\n",
            "\n",
            ", -0.25539 -0.25723 0.13169 -0.042688 0.21817 -0.022702 -0.17854 0.10756 0.058936 -1.3854 0.58509 0.036501 -0.19846 0.19613 0.40929 0.15702 -0.15305 0.050447 0.30045 -0.11295 -0.017043 0.18593 0.19982 0.20053 -0.63141 -0.12622 0.2951 -0.26282 -0.15831 0.0012383 0.011784 0.58758 -0.15914 0.27731 -0.82343 -0.21134 0.013414 0.19637 -0.4147 0.0010276 0.13422 -0.14205 0.051545 0.34993 -0.29868 -0.3209 0.19566 0.47886 0.10744 0.010004 0.18503 0.080694 0.20739 -0.097365 -0.039448 0.020151 -0.17378 0.25679 0.24198 -0.351 0.18759 0.0063857 0.18395 -0.13929 0.0081855 -0.63109 0.29832 0.31731 0.13022 -0.32284 -0.050343 -0.114 0.12097 0.14687 -0.33244 -0.055789 -0.05849 0.27551 -0.043855 0.039664 0.15162 -0.086627 0.067729 0.23146 0.015351 -0.15142 -0.031975 0.45181 -0.068806 -0.077058 0.055193 0.054596 -0.24708 0.031113 -0.12826 0.12782 -0.46708 -0.026264 0.010387 -0.33174 0.17277 -0.26894 0.20467 -0.16181 -0.041519 -0.014878 0.10279 0.18868 -0.23396 -0.018436 -0.14747 -0.32685 -0.022055 -0.054 0.16264 0.27095 -0.22792 -0.0077006 0.11206 -0.039787 -0.11906 0.021773 0.05528 -0.13318 -0.056867 0.008304 -0.027021 0.23447 0.086864 0.12009 -0.30726 0.0024735 0.29041 -0.044887 0.12297 0.13077 0.090807 -0.39141 0.080546 0.18724 -0.097481 0.10397 0.11492 0.17775 -0.18167 0.24652 0.20136 -0.23395 -0.35018 -0.14061 0.17091 -0.095465 -0.10962 -0.09836 0.15344 0.08868 -0.22048 -0.13803 -0.11288 -0.08534 0.072735 -0.12732 -0.1964 -0.10586 0.0020616 0.13496 0.058912 -0.043979 -0.091375 0.24408 0.16872 0.24297 -0.43983 0.47089 -0.018595 0.16146 0.19828 -0.17237 -0.0026998 0.52097 -0.080197 0.43324 -0.066261 0.04324 0.084954 -0.14836 -0.41936 0.15988 -0.18411 0.1321 0.27476 0.27279 -0.13465 -0.091238 -0.32523 0.27936 0.023296 -0.33472 0.016878 -0.055544 0.92915 -0.33914 -0.14791 0.017301 0.18272 0.35108 -0.11438 0.13228 -0.021064 -0.27453 -0.10081 -0.046296 0.21689 -0.056319 0.14651 -0.023536 0.068026 -0.045453 -0.23851 -0.33868 0.31396 -0.031914 -0.019217 0.0018715 -0.13328 0.070148 -0.039761 0.070801 0.0018422 -0.12646 0.028675 -0.095728 0.26673 -0.35536 0.15286 0.064565 0.12647 0.23397 -0.046058 0.13519 -0.14549 0.23031 0.42066 0.16267 -0.16541 -0.0020155 0.080653 -0.30025 -0.076014 0.070612 0.3157 0.05352 -0.10721 -0.1366 0.32214 0.2004 0.11609 -0.22501 0.12155 -0.10851 -0.063187 -0.24553 -0.059751 0.068787 -0.11627 -0.0083402 0.0052044 -0.20159 -0.023663 0.17562 -0.31475 -0.11162 -0.12492 0.10949 -0.26913 0.34893 -1.6997 -0.2447 0.30292 0.05672 -0.31737 0.083612 0.095949 -0.1759 0.10235 0.36808 -0.3438 0.20607 0.19135 0.10992 0.075968 -0.014359 -0.073794 0.22176 0.14652 0.56686 0.053307 -0.2329 -0.12226 0.35499\n",
            "\n",
            ". -0.12559 0.01363 0.10306 -0.10123 0.098128 0.13627 -0.10721 0.23697 0.3287 -1.6785 0.22393 0.12409 -0.086708 0.3301 0.34375 -0.00087582 -0.29658 0.24417 -0.11592 -0.035742 -0.01083 0.20776 0.29285 -0.073491 -0.18598 -0.2009 -0.095366 0.0063732 -0.1362 0.092028 -0.039957 0.19027 -0.10456 0.002767 -0.71742 -0.12915 -0.0013451 0.27002 -0.053023 0.22148 0.13881 -0.15051 -0.1915 0.16402 0.097484 0.056841 0.39789 0.40725 0.14802 0.21569 -0.10671 -0.10232 0.02481 -0.221 -0.01072 0.14234 -0.28242 0.19254 0.08672 -0.3897 0.11321 0.0013779 0.0064009 -0.16206 -0.082153 -0.55397 0.36789 -0.0040159 0.2071 -0.37157 0.25135 -0.19544 -0.047059 0.17155 -0.24036 -0.046086 0.19429 -0.18939 -0.0071974 0.069481 0.059175 -0.17585 0.10653 0.16933 -0.036122 0.029911 -0.1183 0.13916 -0.037951 0.1069 -0.26069 -0.10307 -0.12272 -0.15032 -0.042409 0.013354 -0.2851 0.011248 0.16073 -0.16384 0.21233 -0.18476 -0.00090874 0.066687 0.16918 -0.35004 0.099016 0.46393 -0.19462 0.10346 -0.25668 -0.36516 -0.18963 -0.21933 0.024634 0.065627 -0.1112 -0.164 0.010874 -0.084688 -0.14923 -0.070223 0.028887 0.083497 -0.016193 -0.0024926 0.17186 0.0098749 0.080237 0.14774 0.043206 0.27716 0.57697 -0.041297 0.12765 -0.091517 0.14132 0.087579 0.093224 0.015346 -0.19856 0.017277 -0.10708 -0.013059 -0.37227 0.078568 0.16677 -0.15359 -0.33294 0.036986 0.11697 0.039781 0.038464 -0.16247 0.4128 -0.077491 0.04549 0.1133 0.0082177 -0.25052 0.070966 -0.11388 -0.11503 -0.11014 0.10499 0.15878 -0.27023 -0.011006 0.00076057 0.33902 0.25564 0.16342 -0.56019 0.13055 0.076311 -0.028334 0.28721 -0.027844 -0.11561 0.34925 -0.1242 0.21405 0.24116 -0.031343 0.10913 -0.24755 -0.045429 -0.082178 -0.18831 0.18446 -0.097074 0.32395 0.10658 -0.26676 -0.27311 0.017181 0.25796 -0.28048 0.3079 -0.218 0.87415 -0.12297 0.10991 -0.29797 0.13394 0.10615 -0.10789 -0.35976 -0.18311 -0.45133 0.034967 -0.19847 0.21965 0.08152 0.2581 0.040173 0.031394 0.19069 0.0758 -0.060638 0.20739 0.009839 -0.2693 0.066515 -0.10711 0.0059916 0.23284 -0.058663 0.098993 -0.081464 0.067004 -0.14305 0.25506 -0.31971 -0.03107 -0.092451 0.2944 0.28947 -0.059804 0.24286 -0.16755 0.042031 0.51261 0.24525 -0.65983 0.062456 0.052204 -0.025717 -0.080613 0.080869 0.22821 -0.10217 -0.20719 -0.012123 0.34916 0.086527 0.066288 -0.099828 0.25843 0.11943 -0.13667 -0.43962 0.23704 0.031296 0.074701 -0.22387 0.0078162 -0.19016 0.044444 0.20191 -0.20814 -0.28382 0.10427 -0.21098 0.18865 0.31659 -2.0753 -0.071045 0.52419 0.056023 -0.25295 -0.062168 -0.10989 -0.35755 -0.079244 0.37472 -0.28353 0.16337 0.11165 -0.098002 0.060148 -0.15619 -0.11949 0.23445 0.081367 0.24618 -0.15242 -0.34224 -0.022394 0.13684\n",
            "\n",
            "of -0.076947 -0.021211 0.21271 -0.72232 -0.13988 -0.12234 -0.17521 0.12137 -0.070866 -1.5721 -0.22464 0.04269 -0.4018 0.21006 0.014288 0.41628 0.017165 0.071732 0.0069246 0.18107 -0.15412 0.14933 -0.030493 0.29918 0.029479 -0.036147 -0.061125 0.083918 -0.12398 -0.10077 -0.0054142 0.3371 -0.25612 0.44388 -0.68922 0.1802 0.34898 -0.052284 -0.26226 -0.47109 0.21647 -0.4002 -0.049986 0.011376 0.54994 -0.22791 0.095873 0.47693 -0.056727 -0.17895 0.11756 0.14662 0.048948 0.13587 -0.093821 0.45968 -0.32062 0.29911 0.20656 -0.18503 -0.2769 -0.022545 0.70698 -0.23815 0.16437 -0.55044 -0.0010615 0.12266 0.11898 0.23985 0.29815 0.013207 0.16316 -0.61334 -0.37051 0.19444 -0.13621 -0.30426 -0.37715 0.065299 -0.15995 -0.56516 0.074696 0.40184 0.19328 0.041802 0.20572 0.28971 0.34783 0.33873 -0.10052 -0.16397 -0.15236 -0.086815 0.36522 0.14969 -0.40859 0.23106 0.17162 -0.60545 0.086019 0.37043 0.17937 -0.40282 -0.62471 -0.055919 0.15092 0.12554 -0.45344 0.34417 0.40042 -0.049512 -0.29969 -0.31761 0.30023 0.090029 0.3106 -0.033077 -0.21995 -0.40396 -0.34443 -0.21248 -0.37636 0.21835 -0.1785 -0.17261 0.16391 0.22753 0.2686 0.57541 -0.14912 0.20413 0.22187 -0.27014 0.068253 0.29115 -0.067943 0.10623 -0.16281 0.19939 -0.48613 0.035688 -0.12373 0.13707 0.33359 -0.12713 -0.31711 -0.13962 -0.04288 -0.0014614 0.76883 -0.41705 -0.092911 0.16315 0.29202 0.12119 -0.076683 0.14131 -0.093406 -0.042796 0.13738 0.014278 0.11918 -0.34215 -0.19076 -0.12499 0.24648 0.42259 0.091966 0.45351 0.14437 0.1878 -0.85876 0.059621 -0.32242 0.28627 0.12427 0.0090984 -0.1891 0.16638 0.099881 -0.048553 -0.026257 0.099904 0.12406 -0.015416 -0.29707 -0.4044 -0.17258 0.36468 -0.014118 -0.11889 -0.11686 -0.14124 0.28012 0.067644 0.1485 -0.35702 0.29626 0.36004 1.019 -0.067307 -0.11588 -0.2178 0.070191 0.23154 -0.13849 0.26441 0.28742 0.1941 -0.0060504 0.44105 0.12416 -0.27745 -0.25729 0.10992 0.18362 -0.34522 -0.21861 -0.18825 -0.037454 -0.20862 -0.25216 0.060842 0.068595 0.10275 0.10745 -0.061288 0.19725 -0.27739 -0.022559 0.052794 -0.24083 0.09199 0.30959 0.054999 0.063676 -0.087357 -0.34495 0.22793 -0.42405 0.24536 0.55708 0.19126 -0.797 -0.2048 0.32545 0.09235 0.084791 -0.16433 -0.066568 -0.099249 0.31526 -0.44465 0.087281 0.3288 -0.017809 -0.23855 -0.12848 0.041509 0.46728 0.48214 0.10548 0.065805 0.067221 0.13321 -0.27856 0.015532 0.30026 0.38748 -0.14401 -0.16131 0.17678 0.16448 -0.3244 0.007937 -2.2836 0.096945 0.66131 0.16857 -0.028877 -0.10791 -0.027445 -0.25695 0.046686 0.23087 -0.076458 0.27127 0.25185 0.054947 -0.36673 -0.38603 0.3029 0.015747 0.34036 0.47841 0.068617 0.18351 -0.29183 -0.046533\n",
            "\n",
            "to -0.25756 -0.057132 -0.6719 -0.38082 -0.36421 -0.082155 -0.010955 -0.082047 0.46056 -1.8477 -0.11258 -0.12955 0.27254 0.0072891 0.26038 0.12096 -0.23193 0.03226 -0.29472 -0.67594 -0.33844 -0.23297 0.1102 0.18816 -0.45184 -0.33833 0.11274 0.4949 -0.042132 0.079961 -0.013146 0.062284 0.20223 0.038279 -1.1154 -0.1214 0.089846 0.29702 -0.055794 -0.46021 -0.13194 0.087357 -0.27865 0.14981 0.25536 0.16698 -0.04452 0.067588 -0.11772 -0.13452 0.28694 -0.39844 -0.12806 -0.47818 0.067802 0.20353 -0.30677 0.60789 -0.18588 0.11997 -0.040508 -0.06586 0.30621 -0.055824 0.039448 -0.4557 0.21081 0.25889 0.14666 0.3095 0.14343 0.10524 0.15788 0.103 0.32211 -0.27939 -0.17139 0.32202 0.10784 -0.28209 0.12611 -0.23913 -0.089638 -0.39179 -0.26402 0.36796 -0.23691 0.62503 -0.027226 -0.038851 -0.37359 0.045442 -0.17169 -0.54477 -0.091772 -0.32952 -0.25522 0.087106 0.048685 -0.31684 -0.064198 0.044885 -0.49587 0.429 0.36198 -0.14682 0.20122 -0.030038 0.18736 0.31626 0.059023 -0.13473 -0.40664 -0.52983 0.072459 0.082531 -0.32495 -0.006691 0.43564 -0.12976 -0.26293 0.15975 0.25062 -0.064732 -0.30689 0.16084 0.4583 0.38565 0.12612 -0.00080485 0.18055 0.24757 0.5568 0.21701 0.33105 0.11991 0.0027257 -0.10679 -0.16922 0.25277 0.36024 -0.4762 -0.20035 0.38473 -0.71653 -0.13788 -0.12201 -0.16979 -0.29624 -0.010344 0.19216 0.063375 0.30977 -0.19759 0.57419 0.34018 -0.34795 -0.085304 -0.028243 -0.39182 0.23797 -0.092997 0.11981 -0.2368 0.098179 0.083047 0.12652 0.16026 -0.14166 0.14296 -0.12868 0.18181 -0.42337 0.18061 -0.051635 -0.17987 -0.097071 0.0013755 -0.42539 0.56817 0.057432 0.31577 0.18918 0.27639 -0.39906 0.18594 -0.54231 0.61376 0.37921 0.1366 -0.39842 0.58557 0.054558 0.061801 0.32546 -0.19615 -0.034329 -0.013225 0.67854 -0.16672 0.92439 0.11952 0.040351 0.35368 -0.25573 0.26648 -0.020954 0.18535 0.062376 -0.22976 -0.074563 -0.3289 0.28535 0.46959 -0.54324 0.14124 0.19964 0.1541 0.024155 0.144 0.30989 -0.15989 -0.11611 0.09102 -0.50317 -0.33662 0.059168 0.43838 -0.1428 -0.0053718 0.046505 0.21546 0.065006 -0.35175 -0.17137 0.20467 -0.07173 0.40879 0.20295 -0.014211 -0.21898 -0.12831 0.3224 0.17608 -0.60267 0.036737 0.54848 -0.47682 -0.56556 -0.083633 0.032302 -0.25262 0.39481 -0.019623 0.62547 -0.11369 -0.25727 0.073363 0.18437 0.14587 0.32708 -0.52049 0.037555 0.023667 -0.068237 -0.22916 0.017755 -0.18394 0.55107 -0.23965 0.39187 -0.017785 0.43113 0.27181 -0.16043 -0.347 -2.4194 -0.028952 0.95085 0.05804 -0.23623 0.18914 0.31192 0.23064 -0.30309 -0.18603 0.07618 0.37337 -0.14444 -0.028793 -0.012806 -0.59707 0.31734 -0.25267 0.54384 0.063007 -0.049795 -0.16043 0.046744 -0.070621\n",
            "\n",
            "Total number of word vectors: 399994\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "####2. Retain relevant columns"
      ],
      "metadata": {
        "id": "ZEtzN2B-VaJh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Specify the path to your GloVe word embedding file\n",
        "glove_file_path = '/content/drive/MyDrive/Projects/Project/Project-08/glove.6B/glove.6B.300d.txt'\n",
        "\n",
        "# Define a list of target words you want to retain\n",
        "target_words = ['apple', 'banana', 'orange', 'car', 'house']\n",
        "\n",
        "# Create a dictionary to store word vectors\n",
        "word_vectors = {}\n",
        "\n",
        "# Open the GloVe file and extract word vectors for target words\n",
        "with open(glove_file_path, 'r', encoding='utf-8') as glove_file:\n",
        "    for line in glove_file:\n",
        "        # Split the line into word and vector components\n",
        "        parts = line.strip().split()\n",
        "        word = parts[0]\n",
        "        vector = [float(x) for x in parts[1:]]  # Convert vector components to floats\n",
        "\n",
        "        # Check if the word is in the list of target words\n",
        "        if word in target_words:\n",
        "            word_vectors[word] = vector\n",
        "\n",
        "# Print the retained word vectors\n",
        "for word, vector in word_vectors.items():\n",
        "    print(f\"Word: {word}, Vector: {vector}\")"
      ],
      "metadata": {
        "id": "rrbKYq3MaeZJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7f182fad-f4b2-4274-c740-8fbf6eb24c2a"
      },
      "execution_count": 103,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Word: house, Vector: [-0.3707, -0.081209, -0.44626, 0.097395, 0.19829, -0.041232, 0.21189, 0.22369, -0.59915, -1.3556, -0.0037264, -0.55059, 0.021565, 0.010682, 0.046047, 0.47083, -0.1926, 0.093603, 0.1981, 0.18292, 0.23985, 0.44817, 0.25241, 0.31304, -0.31769, 0.037594, -0.087822, -0.069569, -0.019033, 0.25181, 0.5275, 0.10401, -0.56956, 0.6812, -0.68936, 0.84081, 0.041774, -0.44636, -0.30759, -0.28145, 0.63517, 0.50412, -0.33981, 0.69191, -0.15074, 0.16806, -0.34202, -0.44845, 0.05983, 0.12643, -0.23143, -0.092232, -0.09083, 0.21642, 0.6258, -0.32599, -0.52104, 0.34727, -0.084967, -0.19764, 0.44523, -0.44723, 0.43726, 0.26063, 0.65426, -1.4532, 0.32055, -0.37515, -0.22897, -0.70069, -0.17774, -0.017622, -0.24697, -0.23957, -0.49052, 0.12087, -0.2429, 0.29234, -0.33522, -0.010882, 0.27623, 0.54262, 0.52236, 0.056655, 0.52314, 0.014538, -0.36859, 0.81357, -0.1775, -0.39279, -0.039007, -0.65053, -0.08419, 0.48218, 0.13496, -0.3684, -0.33189, 0.16661, 0.24203, -0.47509, -0.19021, 0.2973, -0.13515, 0.06829, 0.13022, -0.25433, -0.20241, -0.37991, 0.045893, 0.21822, -0.18372, -0.14905, -0.45656, 0.32638, -0.19266, 0.66751, -0.67219, -0.12583, -0.162, -0.71833, 0.053339, -0.0014876, -0.4047, -0.33921, 0.15458, -0.42247, -0.35562, -0.10342, 0.52553, -0.1608, -0.11344, 0.25396, -0.13414, -0.079257, 0.085449, 0.39976, 0.068123, 0.27333, 0.0060912, -0.32324, 0.0047775, 0.09818, -0.28487, 0.68029, -0.55691, -0.45762, 0.14469, -0.20554, -0.35851, -0.1738, 0.47364, 0.66713, -0.10374, -0.41241, -0.14704, 0.37532, 0.06656, -0.016395, 0.18663, -0.45032, 0.53664, 0.034286, -0.32192, -0.34616, -0.23818, 0.44071, -0.47866, -0.29846, 0.3907, 0.24292, 0.083032, -0.23303, -0.23099, 0.3298, -0.078587, 0.4725, -0.13126, -0.41329, 0.40617, 0.058252, -0.2031, -0.47055, 0.19534, 0.34303, 0.037483, -0.10956, 0.037812, 0.32645, 0.15158, 0.43657, 0.18512, -0.30563, -0.33832, -0.38744, -0.028475, 0.58959, 0.38654, 0.20936, -0.39261, -0.38704, 0.79892, 0.033166, 0.088456, 0.043596, 0.077751, -0.49635, 0.15892, -0.20937, -0.31349, -0.17063, 0.072219, 0.0079027, -0.012119, 0.069048, 0.016229, 0.44339, 0.065505, -0.41656, 0.46848, -0.065121, 0.77513, -0.6679, -0.23965, -0.074675, -0.22267, 0.044819, -0.08321, 0.11063, -0.12898, -0.49515, -0.44467, -0.34624, 0.20626, 0.40265, 0.67939, -0.38554, 0.24813, 0.38055, 0.092046, 0.15209, 0.17443, 0.47781, 0.38682, 0.48688, -0.43606, -0.10044, 0.15226, 0.21902, -0.12938, 0.06255, -0.063539, -0.2816, -0.054557, -0.28297, 0.713, -0.095042, -0.13725, -0.016963, 0.76629, 0.25013, 0.035075, 0.21473, -0.17035, -0.16235, -0.34632, 0.10513, 0.52095, 0.22474, 0.54825, 0.056367, -0.12456, -0.35433, -0.10479, -0.2927, -0.18944, -0.018069, -2.3685, 0.26033, 0.51248, 0.14435, -0.40233, -0.019898, 0.085, -0.022858, 0.051306, 0.53513, 0.084923, 0.79667, 0.086744, -0.35761, -0.39222, -0.1203, 0.37379, 0.10174, -0.074873, -0.0099385, -0.28702, 0.030515, -0.33168, 1.0232]\n",
            "Word: car, Vector: [0.46443, 0.3773, -0.21459, -0.50768, -0.24576, 0.08134, 0.10145, 0.25155, -0.36152, -1.603, 0.28219, 0.36653, 0.44611, 0.2795, 0.047722, 0.30087, -0.16226, -0.026055, -0.26815, -0.46282, 0.25012, 0.60389, 0.15111, -0.062823, -0.096755, -0.30548, -0.11376, 0.53914, 0.10966, -0.70618, -0.66316, 0.43559, -0.048631, 0.27755, -0.48685, 0.11938, -0.54538, -0.29563, 0.03447, 0.53187, -0.001588, 0.41692, -0.20742, -0.037833, 0.43333, 0.047521, 0.83507, -0.065088, -0.29974, 0.0047139, 0.12339, -0.5066, 0.2587, 0.21264, 0.19132, 0.54204, -0.11385, -0.42384, -0.27808, -0.15105, -0.62104, 0.27678, -0.054974, 0.018479, -0.11744, 0.33029, -0.35251, -0.21953, 0.05514, 0.16971, -0.36445, 0.36383, 0.20496, 0.66512, -0.2254, -0.30347, -0.52782, -0.66115, 0.11774, -0.14438, 0.33886, 0.2297, 0.63072, 0.76929, 0.02536, -0.13685, 0.084254, 0.32036, -0.10431, 0.7672, 1.0674, 0.42885, 0.03032, -0.63375, 0.70705, -0.17671, -0.1077, -0.31877, 0.063781, -0.57507, -0.40656, 0.55837, 0.72376, 0.2645, 0.20313, -0.43949, 0.53094, 0.075492, -0.26951, -0.4277, -0.13859, 0.28341, 0.13153, -0.32371, 0.22816, 0.43016, -0.015529, 0.13276, 0.048239, -0.038536, 0.013813, -0.24567, 0.4658, -0.61963, -0.39568, -0.21261, 0.11121, -0.19519, 0.32773, -0.25276, 0.87512, 0.4986, 0.25747, 0.46459, -0.097237, 0.42509, 0.0080811, -0.79669, 0.19793, -0.060157, 0.06048, 0.20092, 0.46373, 0.29617, -0.31812, 0.12548, -0.17667, -0.29347, -0.59183, 0.43081, 1.0006, 0.46858, 0.11257, -0.56533, 0.54729, -0.43424, 0.22738, 0.21527, 0.087824, -0.073562, 0.5316, -0.36782, -0.42531, -0.066561, 0.34648, -0.13677, -0.26654, 0.099056, -0.052121, -0.61409, -0.0036171, 0.32633, 0.3905, 0.029416, -0.1119, -0.2118, -0.15688, -0.29308, 0.23114, 0.13873, 0.66445, -0.26318, -0.25518, 0.29499, 0.70132, -0.3239, 0.42109, 0.27316, 0.15021, 0.058074, 0.24201, -0.62194, -0.40832, 0.038272, -0.35372, 0.013934, -0.25013, 0.42916, -0.039463, -0.29794, 1.0178, -0.41685, 0.15442, 0.19598, 0.44718, -0.25764, 0.13438, -0.084223, -0.14142, -0.16001, -0.32541, -0.19588, 0.13934, -0.44924, 0.20827, -0.15747, 0.035537, 0.37097, -0.15336, 0.20401, 0.35981, -0.26311, -0.27436, -0.41229, 0.45533, 0.77527, 0.29339, -0.27392, 0.45037, 0.060015, 0.081354, -0.53969, 0.31872, -0.082837, 0.77918, 0.094676, 0.75256, 0.33377, -0.55349, -0.15661, 0.20791, -0.13498, 0.4756, 0.1218, -0.16496, -0.65669, 0.52394, -0.43666, -0.0087235, 0.52565, -0.12011, -0.21406, -0.15253, -0.22775, 0.59701, -0.64509, -0.445, 0.40937, 0.11943, 0.034467, -0.32471, -0.47915, -0.25264, 0.26818, 0.059601, -0.51842, 0.33124, -0.29623, -0.28343, -0.41471, -0.04505, -0.0017776, -0.37654, -0.023911, 0.55602, 0.2044, -2.2391, 0.14986, -0.098345, 0.18714, -0.19265, -0.0061057, 0.39897, -0.22392, -0.42168, 1.0727, 0.29344, -0.47384, 0.30169, -0.15307, 0.19966, -0.40979, -0.10325, -0.04361, 0.17564, 0.65709, -0.09986, 0.49107, 0.28215, 0.34554]\n",
            "Word: orange, Vector: [-0.24776, -0.12359, 0.20986, -0.15834, -0.15827, -0.90116, -0.095702, -0.23005, 0.27094, -0.18885, -0.6094, -0.29146, -0.14547, -0.17566, 0.7757, 0.23427, -0.7334, -0.34034, -0.56997, -0.046918, 0.024907, -0.18274, 0.14792, 0.18595, -0.046665, -0.55657, -0.38182, 0.21155, -1.1388, 0.24632, -0.2749, -0.21952, -0.69771, 0.05891, -0.37058, 1.0525, -0.19563, -0.17864, -0.24309, 0.021318, -0.055165, -0.27761, -0.21785, 0.52323, 0.16326, -0.54557, 0.0014319, -0.087846, 0.61443, -0.72582, -0.061206, 0.35182, 0.55001, -0.67038, -0.060339, -0.0018221, -0.28547, -0.36713, 0.66334, -0.83004, -0.26006, -0.39763, -0.38772, 0.41361, 0.22863, -0.6484, -0.14765, -0.13465, -0.18154, -0.54855, -0.0749, 0.21009, 0.055898, 0.54107, -0.53956, -0.28911, -0.062195, 0.033496, -0.27442, -0.24379, 0.60123, 0.33735, -0.45934, 0.22089, 0.28856, -0.54551, 0.034183, 0.48993, 0.038962, -0.045273, 0.1544, -0.3798, -0.11647, 0.061435, -0.43743, 0.77289, 0.31942, -0.069738, 0.023407, 0.10732, -0.089489, -0.10486, 0.042569, -0.026284, 0.33343, 0.21235, 0.07399, 0.18305, -0.50787, 0.11003, -0.41798, 0.10016, 0.51145, 0.036357, 0.049747, -0.25304, -0.11242, 0.84524, -0.098083, -0.44179, 0.35958, -0.65623, 0.39122, -0.35646, -0.79987, -0.055275, -0.038645, 0.43572, 0.071285, 0.44192, -0.40933, 0.026579, -0.55951, 0.18181, -0.16561, -0.17277, -0.20476, -0.47509, 0.19047, 0.025337, 0.56109, 0.23522, -0.048615, 0.020294, 0.15802, 0.45451, -0.23932, 0.068622, 0.50821, -0.24706, 0.80493, 0.054989, 0.46609, -0.18083, 0.73783, -0.27206, 0.26418, -0.49461, -0.024973, 0.19331, -0.37615, 0.33468, -0.24353, 0.18171, 0.041497, -0.063388, 0.036015, 0.28952, -0.52935, -0.063903, 0.69211, 0.1215, -0.62731, 0.26627, 0.36438, -0.77108, 0.40561, -0.3078, 0.55919, -0.32124, -0.14276, 0.070346, 0.19114, -0.3929, 0.004516, -0.73871, 0.8074, 0.1354, -0.12414, -0.79568, 0.058827, 0.48552, -0.2902, -0.11301, 0.018115, -0.16892, -0.42663, -0.2062, -0.21115, 0.42126, 0.91802, 0.07018, -0.010133, -0.16455, 0.21174, 0.14027, 0.52289, 0.75919, 0.050752, 0.75467, -0.007933, 0.23611, -0.68522, -0.42567, 0.34091, 0.020985, 1.0518, -0.031345, -0.086061, -0.57768, 1.3026, -0.37067, -0.35319, 0.052281, -0.16382, -0.46568, -0.10996, -0.42071, -0.50616, -0.50337, -0.31583, 0.47216, -0.053558, 0.15372, 0.31001, -0.10349, 0.17909, -0.32471, -0.16194, -0.53479, -0.13764, -0.71118, 0.18695, 0.16932, -0.73799, 0.037364, 1.0209, -0.42802, 0.13443, -0.35761, 0.39819, 0.43802, -0.16397, -0.15303, 0.27137, -0.5281, 0.31447, -0.11779, 0.2753, -0.20477, 0.19547, 0.14832, 0.7838, -0.34746, -0.5382, -0.74684, -0.30637, -0.23923, -0.1677, 0.30751, -0.11028, -0.0076799, -0.21395, -0.055122, -0.44104, 0.13752, -1.3948, -0.11891, -0.36035, 0.051432, 0.0089858, -0.029622, -0.20384, -0.235, -0.065453, 0.54828, -0.33196, -0.11157, -0.0050479, 0.13196, 0.33844, -0.15593, -0.10277, -0.082953, 0.43793, -0.22457, 0.31513, 0.079717, 0.23865, -0.014213]\n",
            "Word: apple, Vector: [-0.20842, -0.019668, 0.063981, -0.71403, -0.21181, -0.59283, -0.15316, 0.044217, 0.63289, -0.84821, -0.21129, -0.19763, 0.19029, -0.56226, 0.27126, 0.23782, -0.5189, -0.24518, 0.035243, 0.096833, 0.24898, 0.71279, 0.038279, -0.10514, -0.4779, -0.39515, -0.27194, -0.44428, 0.06113, -0.2318, -0.35901, -0.18239, 0.035507, -0.087719, -1.0816, -0.42521, 0.003224, -0.45991, -0.043462, -0.39031, 0.519, 0.21139, -0.25527, 1.1805, -0.19041, -0.12156, 0.034186, -0.062316, 0.14421, -0.53366, 0.47425, -0.4471, 0.58047, 0.43578, 0.1321, -0.095712, -0.37182, -0.013837, 0.20601, -0.10099, 0.10685, -0.33723, 0.10986, 0.34796, -0.099839, 0.36942, -0.52917, 0.12407, -0.46127, -0.38483, -0.10114, -0.17634, 0.37574, 0.16377, -0.2198, -0.26841, 0.84706, -0.35619, -0.083992, -0.20276, -0.56542, 0.19112, -0.14134, -0.7812, 0.69188, -0.083628, -0.54293, 0.16437, 0.037606, -0.68896, -0.68711, -0.13367, -0.4779, 0.20125, 0.085122, -0.063865, -0.17104, -0.32432, -0.17623, -0.514, -0.50289, 0.23204, -0.11324, -1.064, -0.035359, -0.5068, -0.27118, -0.16621, -0.63016, 0.054252, -0.048178, 0.29282, -0.030666, -0.24645, -0.27084, -0.42563, -0.39171, 0.18428, -0.017772, -0.35334, -0.49075, -0.90782, 0.13872, -0.76521, -0.46318, -0.32124, -0.086228, 1.0448, -0.39919, 0.69478, -0.10377, 0.86715, 0.22742, 0.4384, 0.085767, -0.22846, 0.4309, 0.064187, -0.027926, -0.093056, 0.65188, 0.59143, -0.3376, -0.37732, 0.0052212, 1.1193, -0.23845, -0.16029, 0.42877, -0.16228, -0.12202, -0.1061, 0.015761, 0.022745, -0.17734, -0.091711, -0.29158, 0.19034, -0.35168, 0.27563, -0.20577, 0.11472, -0.34126, -0.0065915, 0.14896, -0.026762, 0.0019373, 0.53279, -0.76088, 0.063085, -0.72089, -0.04128, -0.96164, 0.020769, 0.16123, -0.34342, 0.69713, -0.16018, -0.11701, -0.070239, -0.30774, 0.39741, 0.39994, -0.678, 0.57684, -0.48099, 0.59317, -0.42262, 0.28613, -0.26203, 0.052727, 0.61659, -0.36801, -0.28429, -0.40054, -0.30055, -0.27444, -0.045729, -0.56105, 0.24176, 0.86631, -0.83715, 0.13562, 0.26196, -0.43055, 0.34558, 0.059441, 0.61845, 0.11837, -0.019168, 0.47697, -0.32465, -0.15463, -0.23556, -0.64263, -0.092156, -0.19622, 0.40666, 0.18009, 0.094309, 0.046917, 0.26369, -0.50727, 0.37491, -0.66773, 0.35095, -0.033835, 0.30534, 0.23166, 0.023526, -0.68365, 0.26078, -0.22526, -0.2656, 0.59967, 0.2598, 0.36248, 0.15564, -0.45549, 0.11153, -0.33287, 0.081364, -0.36989, -0.25543, -1.1628, -0.14622, -0.032971, -0.55619, 0.47717, -0.29021, 0.42688, 1.2397, -0.81391, 0.21084, -0.25426, -0.08684, -0.078412, 0.26035, 0.3281, -0.23777, 0.05138, -0.030247, -0.15669, 0.057147, 0.33902, 0.12795, -0.21468, -0.75208, 0.41422, 0.0062719, -0.52904, 0.92193, -0.42179, -0.69638, 0.074115, 0.19071, -1.2031, -0.081333, -0.4914, -0.22159, -0.29876, 0.30094, 0.018634, 0.18786, -0.45429, -0.29296, 0.3695, -0.24218, -0.11803, 0.071775, 0.44026, -0.59978, 0.45354, 0.17854, -0.17155, 0.018811, -0.62354, -0.014163, 0.16799, -0.064392]\n",
            "Word: banana, Vector: [0.42141, 0.020467, 0.12666, 0.39762, -0.11016, -0.035956, -0.47214, -0.13916, 0.56812, -0.34969, -0.093232, -0.17035, -0.38677, -0.16811, -0.10157, -0.26612, 0.048094, -0.46771, -0.60725, 0.40952, 0.31771, 0.50098, 0.66368, -0.11827, -0.74267, -0.10472, -0.64353, -0.44023, -0.39101, 0.35694, -0.93489, 0.48317, 0.15223, 0.079339, -0.25111, 0.39968, -0.17982, -0.28874, -0.10891, 0.38821, -0.23147, -0.50337, -0.25231, -0.022184, -0.27874, -0.24193, 0.057466, -0.53955, -0.034875, -0.40482, -0.038067, -0.42337, 0.42861, 0.35166, -0.18165, -0.31131, -0.53276, -0.050954, 0.66779, -0.40077, 0.21403, -0.29861, -0.36637, 0.28489, -0.37663, 0.059604, -0.31795, 0.25463, -0.22185, 0.23032, -0.12302, 0.24175, -0.10706, -0.086599, -0.037363, -0.10403, 0.24492, -0.84063, -0.1535, -0.19363, -0.018541, 0.10938, -0.29402, -0.11271, -0.38886, -0.42836, -0.4486, -0.24651, -0.094971, -0.63275, 0.20591, -0.77712, -0.23888, -0.79994, -0.36994, 0.37863, 0.27856, -0.10061, 0.064728, 0.091214, 0.24322, 0.39319, 0.27138, -0.6939, -0.37603, 0.19322, -0.28888, -0.013833, -0.20092, 0.20651, 1.1443, 0.20413, 0.077503, 0.36868, 0.26764, -0.19205, 0.12437, 0.7254, -0.40392, 0.20171, 0.027611, -0.70727, 0.73353, -0.28918, -0.076865, 0.16481, 0.4793, 1.0438, -0.012671, 0.2165, -0.54563, 0.74603, 0.050535, 0.43028, 0.28583, -0.22623, -0.10048, 0.021873, -0.015193, -0.36967, -0.01258, -0.033953, -0.087644, 0.067809, 0.075793, 0.77514, 0.36431, -0.31494, 0.44823, -0.49692, -0.39522, 0.172, 0.32744, 0.2807, -0.22451, 0.016309, -0.49604, -0.070684, 0.44588, 0.69836, 0.57786, -0.086133, 0.0885, -0.13028, -0.47819, -0.5681, -0.35058, 0.45372, -0.077019, -0.39147, -0.0062599, -0.008847, -0.55888, -0.27865, 0.45821, 0.040499, 0.096597, 0.79329, -0.0081883, -0.22361, 0.13949, 0.063997, -0.04814, -0.8997, 0.32938, -0.73243, 0.4952, 0.43429, 0.39592, -0.36047, -0.44325, 1.1874, -0.14529, -0.24614, 0.16348, 0.24299, -0.086866, -0.3142, -0.10316, 0.44774, 0.12476, 0.29402, 0.056585, -0.0081334, 0.41448, 0.078065, 0.42369, 0.59514, -0.18196, -0.11808, -0.16229, -0.37044, -0.45505, 0.23213, -0.18806, -0.057701, 0.35678, -0.29694, 0.40711, 0.29314, 0.50997, -0.4906, -0.038442, 0.27699, -0.17814, 0.56196, -0.25775, 0.16302, -0.12892, 0.18511, 0.044475, -0.050004, 0.003403, 0.72436, 0.72849, -0.050832, 0.63507, -0.51974, -0.018575, -0.040821, -0.065155, -0.4737, 0.03105, -0.2919, -1.0685, 0.19157, 0.35104, 0.65448, 0.094602, -0.74953, 0.27771, 0.85203, -0.13938, -0.0026958, 0.75905, 0.1525, -0.18057, 0.35792, 0.22094, -0.0026224, 0.24357, -0.044449, 0.024307, -0.18554, 0.5473, 0.067562, -0.17508, -0.4967, 0.191, -0.12052, 0.00056214, -0.042702, 0.083795, 0.4198, 0.14462, 0.14566, -0.33083, -0.25936, -0.58499, 0.082745, -0.49894, -0.24473, -0.31759, -0.6223, -0.4137, 0.10231, 0.56253, -0.41108, 0.15782, 0.0936, -0.066787, -0.6505, 0.43919, -0.07727, -0.10359, 0.20172, -0.64019, 0.093869, 0.23952, 0.3014]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "####3. Get length of each sentence"
      ],
      "metadata": {
        "id": "Hqy84VuqjSCt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "\n",
        "# Read the content of the text file\n",
        "file_path = '/content/drive/MyDrive/Projects/Project/Project-08/glove.6B/glove.6B.300d.txt'\n",
        "\n",
        "with open(file_path, 'r', encoding='utf-8') as file:\n",
        "    text_data = file.read()\n",
        "\n",
        "# Tokenize the text data into sentences\n",
        "sentences = sent_tokenize(text_data)\n",
        "\n",
        "# Create a list to store the lengths of sentences\n",
        "sentence_lengths = []\n",
        "\n",
        "# Iterate through each sentence\n",
        "for sentence in sentences:\n",
        "    # Tokenize the sentence into words\n",
        "    words = word_tokenize(sentence)  # You might need more advanced tokenization based on your data\n",
        "\n",
        "    # Calculate the length of the sentence (number of words)\n",
        "    sentence_length = len(words)\n",
        "\n",
        "    # Append the sentence length to the list\n",
        "    sentence_lengths.append(sentence_length)\n",
        "\n",
        "# Now 'sentence_lengths' contains the length of each sentence\n",
        "# You can use this list for further analysis or visualization"
      ],
      "metadata": {
        "id": "73ruhZNYgPEi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3ffa95b8-5226-4bc1-ea2b-843342a05363"
      },
      "execution_count": 112,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "####4. Define parameters"
      ],
      "metadata": {
        "id": "lSSqPXwMj9jF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To define parameters for a natural language processing (NLP) task, you'll typically need to consider various aspects of your data and the NLP model you plan to use. Here are some common parameters to define for an NLP task:\n",
        "\n",
        "**Vocabulary Size:** Decide on the size of your vocabulary. This determines the number of unique words that your model will consider. A larger vocabulary allows your model to capture more words but can increase complexity.\n",
        "\n",
        "**Sequence Length:** Determine the maximum sequence length for your input data. Sequences longer than this length will be truncated, and shorter sequences will be padded.\n",
        "\n",
        "**Embedding Dimension:** Choose the dimensionality of your word embeddings. Common values include 50, 100, 200, or 300. Pre-trained word embeddings like Word2Vec or GloVe often come in these dimensions.\n",
        "\n",
        "**Model Architecture:** Decide on the architecture of your NLP model. This might be a simple recurrent neural network (RNN), a more complex model like LSTM or GRU, or a deep learning architecture like a Transformer.\n",
        "\n",
        "**Batch Size:** Specify the batch size for training. Smaller batch sizes require less memory but may slow down training. Larger batch sizes can speed up training but require more memory.\n",
        "\n",
        "**Epochs:** Choose the number of training epochs. An epoch is one complete pass through your training data. Training for too many epochs can lead to overfitting.\n",
        "\n",
        "**Learning Rate:** Set the learning rate for your optimizer. This parameter controls the step size during optimization. Learning rates that are too high can cause overshooting, while rates that are too low can slow down convergence.\n",
        "\n",
        "**Optimizer:** Select an optimizer algorithm, such as Adam, SGD, or RMSprop.\n",
        "\n",
        "**Loss Function:** Choose an appropriate loss function based on your task (e.g., categorical cross-entropy for classification).\n",
        "\n",
        "**Metrics:** Define evaluation metrics for your model, such as accuracy, F1-score, or custom metrics specific to your problem.\n",
        "\n",
        "**Dropout:** Decide whether to use dropout layers to prevent overfitting. Specify the dropout rate if used.\n",
        "\n",
        "**Regularization:** Consider using L1 or L2 regularization to reduce overfitting.\n",
        "\n",
        "**Early Stopping:** Implement early stopping to prevent training for too many epochs and overfitting.\n",
        "\n",
        "**Model Initialization:** Decide whether to use pre-trained embeddings or initialize your embeddings randomly.\n",
        "\n",
        "**Fine-tuning:** If using pre-trained embeddings, determine whether you'll fine-tune them during training or keep them fixed.\n",
        "\n",
        "**Other Hyperparameters:** Depending on the specific NLP task, there may be additional hyperparameters to tune, such as attention mechanisms for sequence-to-sequence tasks.\n",
        "\n",
        "Here's an example of how you might define these parameters in code using TensorFlow and Keras:"
      ],
      "metadata": {
        "id": "zfIcmS93toYE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vocab_size = 10000  # Size of vocabulary\n",
        "max_sequence_length = 100  # Maximum sequence length\n",
        "embedding_dim = 100  # Dimensionality of word embeddings\n",
        "model_architecture = 'LSTM'  # Choose your model architecture\n",
        "batch_size = 32  # Batch size for training\n",
        "epochs = 10  # Number of training epochs\n",
        "learning_rate = 0.001  # Learning rate for optimizer\n",
        "optimizer = 'adam'  # Optimizer algorithm\n",
        "loss_function = 'categorical_crossentropy'  # Loss function for training\n",
        "metrics = ['accuracy']  # Evaluation metrics\n",
        "dropout_rate = 0.2  # Dropout rate for regularization"
      ],
      "metadata": {
        "id": "erWHIfZpkGZU"
      },
      "execution_count": 113,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "These parameters serve as a starting point, and you may need to experiment with different values to find the best configuration for your specific NLP task. Additionally, you can use techniques like grid search or random search to search for optimal hyperparameters."
      ],
      "metadata": {
        "id": "jc05085FupLr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "####5. Get indices for words"
      ],
      "metadata": {
        "id": "P1hCWdqYkS7k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "\n",
        "# Example text data\n",
        "text_data = [\"This is a sentence.\", \"Another sentence.\", \"And one more sentence.\"]\n",
        "\n",
        "# Create a tokenizer\n",
        "tokenizer = Tokenizer()\n",
        "\n",
        "# Fit the tokenizer on the text data to build the vocabulary\n",
        "tokenizer.fit_on_texts(text_data)\n",
        "\n",
        "# Get the word-to-index mapping (word index)\n",
        "word_to_index = tokenizer.word_index\n",
        "\n",
        "# Get the index-to-word mapping (index word)\n",
        "index_to_word = {index: word for word, index in word_to_index.items()}\n",
        "\n",
        "# Example: Getting the index of a word\n",
        "word = \"sentence\"\n",
        "word_index = word_to_index.get(word)\n",
        "print(f\"Index of '{word}': {word_index}\")\n",
        "\n",
        "# Example: Getting the word from an index\n",
        "index = 3\n",
        "word = index_to_word.get(index)\n",
        "print(f\"Word at index {index}: {word}\")"
      ],
      "metadata": {
        "id": "d7xu-N1hsSaK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9610c28c-660b-40e0-dd7c-2b09da141333"
      },
      "execution_count": 114,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Index of 'sentence': 1\n",
            "Word at index 3: is\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "####6. Create features and labels"
      ],
      "metadata": {
        "id": "FpCrGdeSsMTm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Example text data and corresponding labels\n",
        "text_data = [\"This is a positive sentence.\", \"Another positive example.\", \"This is a negative sentence.\", \"Another negative example.\"]\n",
        "labels = [1, 1, 0, 0]  # 1 for positive, 0 for negative\n",
        "\n",
        "# You can also use text labels like this:\n",
        "# labels = [\"positive\", \"positive\", \"negative\", \"negative\"]\n",
        "\n",
        "# Tokenize the text data (assuming you have already fit a tokenizer)\n",
        "# Replace 'tokenizer' with your actual tokenizer\n",
        "sequences = tokenizer.texts_to_sequences(text_data)\n",
        "\n",
        "# Pad sequences to a fixed length (adjust maxlen as needed)\n",
        "maxlen = 10  # Example: Set to the desired sequence length\n",
        "padded_sequences = pad_sequences(sequences, maxlen=maxlen, padding='post', truncating='post')\n",
        "\n",
        "# Now, 'padded_sequences' contains your feature data (input data)\n",
        "# 'labels' contains your target labels (output data)"
      ],
      "metadata": {
        "id": "nl71EWCclilq"
      },
      "execution_count": 115,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "####7. Get vocabulary size."
      ],
      "metadata": {
        "id": "GKz9A0FR4Hy2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Assuming you have already fit a tokenizer on your text data\n",
        "# Replace 'tokenizer' with your actual tokenizer\n",
        "vocab_size = len(tokenizer.word_index) + 1  # Add 1 for the padding token if used\n",
        "\n",
        "print(f'Vocabulary size: {vocab_size}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "efb15uUm46Az",
        "outputId": "4fb626a9-848c-4f52-d553-f0da35ce44b1"
      },
      "execution_count": 116,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vocabulary size: 9\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "####8. Create a weight matrix using GloVe embeddings."
      ],
      "metadata": {
        "id": "bBwXaZCvsswZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: Specify the path to your GloVe word vectors file\n",
        "glove_path = '/content/drive/MyDrive/Projects/Project/Project-08/glove.6B/glove.6B.50d.txt'\n",
        "\n",
        "# Step 2: Load the GloVe word vectors into a DataFrame\n",
        "# You can specify the delimiter as space to separate columns\n",
        "glove_data = pd.read_csv(glove_path, sep=\" \", header=None, quoting=3)\n",
        "\n",
        "# Step 3: Explore the GloVe word vectors\n",
        "# You can view the first few rows of the DataFrame to understand its structure\n",
        "print(glove_data.head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gG9EEzlAyX8i",
        "outputId": "a5b7be4e-e523-4c43-9d28-2726c3ae0266"
      },
      "execution_count": 121,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    0         1         2        3        4        5         6        7   \\\n",
            "0  the  0.418000  0.249680 -0.41242  0.12170  0.34527 -0.044457 -0.49688   \n",
            "1    ,  0.013441  0.236820 -0.16899  0.40951  0.63812  0.477090 -0.42852   \n",
            "2    .  0.151640  0.301770 -0.16763  0.17684  0.31719  0.339730 -0.43478   \n",
            "3   of  0.708530  0.570880 -0.47160  0.18048  0.54449  0.726030  0.18157   \n",
            "4   to  0.680470 -0.039263  0.30186 -0.17792  0.42962  0.032246 -0.41376   \n",
            "\n",
            "        8        9   ...        41        42        43        44       45  \\\n",
            "0 -0.17862 -0.00066  ... -0.298710 -0.157490 -0.347580 -0.045637 -0.44251   \n",
            "1 -0.55641 -0.36400  ... -0.080262  0.630030  0.321110 -0.467650  0.22786   \n",
            "2 -0.31086 -0.44999  ... -0.000064  0.068987  0.087939 -0.102850 -0.13931   \n",
            "3 -0.52393  0.10381  ... -0.347270  0.284830  0.075693 -0.062178 -0.38988   \n",
            "4  0.13228 -0.29847  ... -0.094375  0.018324  0.210480 -0.030880 -0.19722   \n",
            "\n",
            "         46        47        48        49       50  \n",
            "0  0.187850  0.002785 -0.184110 -0.115140 -0.78581  \n",
            "1  0.360340 -0.378180 -0.566570  0.044691  0.30392  \n",
            "2  0.223140 -0.080803 -0.356520  0.016413  0.10216  \n",
            "3  0.229020 -0.216170 -0.225620 -0.093918 -0.80375  \n",
            "4  0.082279 -0.094340 -0.073297 -0.064699 -0.26044  \n",
            "\n",
            "[5 rows x 51 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Filter out rows with missing or invalid values in the first column\n",
        "actual_data = glove_data[glove_data.iloc[:, 0].apply(lambda x: isinstance(x, str))]\n",
        "\n",
        "# Reset the index to start from 0\n",
        "actual_data.reset_index(drop=True, inplace=True)\n",
        "\n",
        "# Print the first few rows of the actual data\n",
        "print(actual_data.head())\n",
        "\n",
        "# Step 5: Get the length of each sentence (optional, assuming your text data is in sentences)\n",
        "# Assuming your text data is in the first column (column 0)\n",
        "sentence_lengths = actual_data.iloc[:, 0].apply(lambda x: len(x.split()))\n",
        "\n",
        "# Print the first few sentence lengths (optional)\n",
        "print(sentence_lengths.head())\n",
        "\n",
        "# Step 6: Define parameters (if you're building a neural network)\n",
        "vocab_size = len(actual_data)  # Vocabulary size\n",
        "embedding_dim = len(glove_data.columns) - 1  # Embedding dimension\n",
        "\n",
        "# Now, you have 'actual_data' which contains the valid words and 'sentence_lengths' (optional) for sentence lengths.\n",
        "# You can proceed with further processing, such as creating features and labels or building a word embedding matrix for your NLP task."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hFv2wTG2zV7x",
        "outputId": "5f14a786-b7a3-4b17-c558-f1ee38a139cd"
      },
      "execution_count": 125,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    0         1         2        3        4        5         6        7   \\\n",
            "0  the  0.418000  0.249680 -0.41242  0.12170  0.34527 -0.044457 -0.49688   \n",
            "1    ,  0.013441  0.236820 -0.16899  0.40951  0.63812  0.477090 -0.42852   \n",
            "2    .  0.151640  0.301770 -0.16763  0.17684  0.31719  0.339730 -0.43478   \n",
            "3   of  0.708530  0.570880 -0.47160  0.18048  0.54449  0.726030  0.18157   \n",
            "4   to  0.680470 -0.039263  0.30186 -0.17792  0.42962  0.032246 -0.41376   \n",
            "\n",
            "        8        9   ...        41        42        43        44       45  \\\n",
            "0 -0.17862 -0.00066  ... -0.298710 -0.157490 -0.347580 -0.045637 -0.44251   \n",
            "1 -0.55641 -0.36400  ... -0.080262  0.630030  0.321110 -0.467650  0.22786   \n",
            "2 -0.31086 -0.44999  ... -0.000064  0.068987  0.087939 -0.102850 -0.13931   \n",
            "3 -0.52393  0.10381  ... -0.347270  0.284830  0.075693 -0.062178 -0.38988   \n",
            "4  0.13228 -0.29847  ... -0.094375  0.018324  0.210480 -0.030880 -0.19722   \n",
            "\n",
            "         46        47        48        49       50  \n",
            "0  0.187850  0.002785 -0.184110 -0.115140 -0.78581  \n",
            "1  0.360340 -0.378180 -0.566570  0.044691  0.30392  \n",
            "2  0.223140 -0.080803 -0.356520  0.016413  0.10216  \n",
            "3  0.229020 -0.216170 -0.225620 -0.093918 -0.80375  \n",
            "4  0.082279 -0.094340 -0.073297 -0.064699 -0.26044  \n",
            "\n",
            "[5 rows x 51 columns]\n",
            "0    1\n",
            "1    1\n",
            "2    1\n",
            "3    1\n",
            "4    1\n",
            "Name: 0, dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "####9. Define and compile a Bidirectional LSTM model.\n",
        "**Hint:** Be analytical and experimental here in trying new approaches to design the best model."
      ],
      "metadata": {
        "id": "kBLl_fB_0_Rs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, Bidirectional, LSTM, Dense\n",
        "\n",
        "# 1. Prepare your text data\n",
        "# Load your text data into a DataFrame (replace 'your_data.csv' with your data file)\n",
        "data = pd.read_csv('/content/drive/MyDrive/Projects/Project/Project-08/imdb_10K_sentiments_reviews.csv')\n",
        "\n",
        "# 2. Tokenize your text data (adjust this step based on your data)\n",
        "tokenized_sentences = [sentence.split() for sentence in data['text_column']]\n",
        "\n",
        "# 3. Define your vocabulary size (adjust as needed)\n",
        "vocab_size = 10000\n",
        "\n",
        "# 4. Load GloVe word embeddings (adjust the file path and embedding dimension)\n",
        "embedding_dim = 50\n",
        "glove_file = '/content/drive/MyDrive/Projects/Project/Project-08/glove.6B/glove.6B.50d.txt'\n",
        "\n",
        "# Load the GloVe embeddings into a dictionary\n",
        "embeddings_index = {}\n",
        "with open(glove_file, 'r', encoding='utf-8') as f:\n",
        "    for line in f:\n",
        "        values = line.split()\n",
        "        word = values[0]\n",
        "        coefs = np.asarray(values[1:], dtype='float32')\n",
        "        embeddings_index[word] = coefs\n",
        "\n",
        "# 5. Load your own data (already done in step 1)\n",
        "\n",
        "# 6. Calculate the maximum sentence length\n",
        "max_sentence_length = max(len(sentence) for sentence in tokenized_sentences)\n",
        "\n",
        "# 7. Tokenize your text data\n",
        "tokenizer = Tokenizer(num_words=vocab_size)\n",
        "tokenizer.fit_on_texts(data['text_column'])\n",
        "sequences = tokenizer.texts_to_sequences(data['text_column'])\n",
        "\n",
        "# 8. Pad sequences\n",
        "padded_sequences = pad_sequences(sequences, maxlen=max_sentence_length, padding='post', truncating='post')\n",
        "\n",
        "# 9. Create labels (if applicable)\n",
        "# labels = ...\n",
        "\n",
        "# 10. Define and compile your model\n",
        "model = Sequential()\n",
        "model.add(Embedding(input_dim=vocab_size, output_dim=embedding_dim, input_length=max_sentence_length))\n",
        "model.add(Bidirectional(LSTM(128)))  # Adjust the LSTM units as needed\n",
        "model.add(Dense(1, activation='sigmoid'))  # Adjust for your classification task\n",
        "\n",
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "# Train your model using padded_sequences and labels (if applicable)\n",
        "\n",
        "# Evaluate your model, make predictions, etc."
      ],
      "metadata": {
        "id": "vuubR9y44cpi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "####10. Fit the model and check the validation accuracy."
      ],
      "metadata": {
        "id": "bs8WDUly1ZZH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenize your text data (adjust this step based on your data)\n",
        "tokenized_sentences = [sentence.split() for sentence in data['text_column']]"
      ],
      "metadata": {
        "id": "TWJsFDHLBuSU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate the maximum sentence length\n",
        "max_sentence_length = max(len(sentence) for sentence in tokenized_sentences)"
      ],
      "metadata": {
        "id": "yoFGYVkbDm1P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate the maximum sentence length\n",
        "max_sentence_length = max(len(sentence) for sentence in tokenized_sentences)\n",
        "\n",
        "# Define and compile your model\n",
        "model = Sequential()\n",
        "model.add(Embedding(input_dim=vocab_size, output_dim=embedding_dim, input_length=max_sentence_length))\n",
        "model.add(Bidirectional(LSTM(128)))  # Adjust the LSTM units as needed\n",
        "model.add(Dense(1, activation='sigmoid'))  # Adjust for your classification task\n",
        "\n",
        "# Compile the model with an optimizer and loss function\n",
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "# Split your data into training and validation sets\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Split the data into, for example, 80% for training and 20% for validation\n",
        "X_train, X_val, y_train, y_val = train_test_split(padded_sequences, labels, test_size=0.2, random_state=42)\n",
        "\n",
        "# Fit the model to the training data and specify validation data\n",
        "history = model.fit(X_train, y_train, epochs=10, batch_size=64, validation_data=(X_val, y_val))\n",
        "\n",
        "# Check the validation accuracy\n",
        "val_accuracy = history.history['val_accuracy']\n",
        "print(f'Validation Accuracy: {val_accuracy[-1]}')"
      ],
      "metadata": {
        "id": "z9Cu81A91jOO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Project is Done**\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "##**Thank You.**"
      ],
      "metadata": {
        "id": "aH4yHSLGyJT3"
      }
    }
  ]
}